---
authors:
- admin
#- Âê≥ÊÅ©ÈÅî
categories:
#- Demo
#- ÊïôÁ®ã
date: "2021-11-18T00:00:00Z"
draft: false
featured: false
image:
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/CpkOjOcXdUY)'
  focal_point: ""
  placement: 2
  preview_only: false
lastmod: "2021-11-18T00:00:00Z"
projects: []
subtitle: "The inclusion guide"
#summary: "Welcome \U0001F44B We know that first impressions are important, so we've
#  populated your new site with some initial content to help you get familiar with
#  everything in no time."
tags:
#- Academic
#- ÂºÄÊ∫ê
title: For a better understanding of what Artificial Intelligence is and how it can be used (or misused) in education and training

youtubeId2: 7n9IOH0NvyY
---

![](inclusion.jpg)
## Why does FAIaS create an inclusion guide?
When an homogeneous group is creating the AI algorithms, this translates into the creation of algorithms that (can unconsciously) discriminate unethically. 

To attend the event and workshops, register [**here**](https://docs.google.com/forms/d/e/1FAIpQLSenWv6oMdX3Vj_9EcOAVMHPZP57SuhP3lZVIUTscCsJHKM2nw/viewform)

### Racial discrimination

![](My_Color_is_not_a_crime.jpg)

The first and most famous case, the COMPAS model, shows how even the simplest models can discriminate unethically according to race.

Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, from [**ProPublica**](https://www.propublica.org/) published on May 23, 2016 the article called ‚Äò[**Machine Bias**](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)‚Äô that states that [**the artificial intelligence software COMPAS**](https://en.wikipedia.org/wiki/COMPAS_(software)) (Correctional Offender Management Profiling for Alternative Sanctions) used in courtrooms across the United States to predict future crimes is biased against Black defendants.
Further research has been done on the topic including criticism of the original article.

Some extra references about the COMPAS model can be found here:
* An article published on Towards Data Science with title: ‚Äò[**COMPAS Case Study: Fairness of a Machine Learning Model**](https://towardsdatascience.com/compas-case-study-fairness-of-a-machine-learning-model-f0f804108751)‚Äô
* A Massive Science article with title: ‚Äò[**Can the criminal justice system‚Äôs artificial intelligence ever be truly fair? Computer programs used in 46 states incorrectly label Black defendants as ‚Äúhigh-risk‚Äù at twice the rate as white defendants**](https://massivesci.com/articles/machine-learning-compas-racism-policing-fairness/)
* A criticism on the original ProPublica article with title:  [**False Positives, False Negatives, and False Analyses: A Rejoinder to ‚ÄúMachine Bias: There‚Äôs Software Used Across the Country to Predict Future Criminals. And it‚Äôs Biased Against Blacks**](https://www.researchgate.net/publication/306032039_False_Positives_False_Negatives_and_False_Analyses_A_Rejoinder_to_Machine_Bias_There's_Software_Used_Across_the_Country_to_Predict_Future_Criminals_And_it's_Biased_Against_Blacks) 

### Gender discrimination

![](ai.jpg)

An example of how the creation of algorithms that discriminate is affecting women, and will affect girls is recruitment. More and more the process of selection of candidates for a job
and the recruitment process is done via digital systems with AI algorithms behind. This means that the selection of candidates is biased and some groups are being discriminated
by the algorithm and will not even have the chance to get to an interview. 
Some references can be found here:

* A Reuters article explains how and why AI recruiting tools can be biased against women with the use case of [**Amazon‚Äôs automated recruitment system**](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
* A Bloomberg article from Bass, D. and Huet, E. (2017) that show how [**Researchers Combat Gender and Racial Bias in Artificial Intelligence**](https://www.bloomberg.com/news/articles/2017-12-04/researchers-combat-gender-and-racial-bias-in-artificial-intelligence).
* A Medium Article from 2018 about ‚Äò[**Racial bias and gender bias in AI systems with examples**](https://medium.com/thoughts-and-reflections/racial-bias-and-gender-bias-examples-in-ai-systems-7211e4c166a1)‚Äô that also included the COMPAS use case.

With the previous references and examples about bias in AI related to race and gender, we can clearly see that the bias challenges need to be tackle to guarantee that our systems are not automating tendencies or discrimination. Bias should be important for everyone, it is not only about women‚Äôs jobs, but about many more aspects that may affect men too.  Example: As proportionately more than 90% of [**murders were commited by men**](https://www.ojjdp.gov/ojstatbb/crime/ucr.asp?table_in=1&selYrs=2019&rdoGroups=1&rdoData=c) in the last years, and [**most teachers are female**](https://nces.ed.gov/programs/coe/indicator/clr), AI systems that are designed with poor knowledge and consciousness about bias will tend to automate discriminatory practices against men and classify them easily as ‚Äòpotential murders‚Äô or as ‚Äòpotentially unfit for teaching jobs‚Äô.  Will we allow this discriminatory systems against our men?  Gender and racial discrimination should be important for everyone and the FAIaS project cares about it.  For inspiration check the Michael Kimmel‚Äôs TED talk on ‚Äò[**Why gender equality is good for everyone - Men included**](https://youtu.be/7n9IOH0NvyY)‚Äô

{% include youtubePlayer.html id=page.youtubeId2 %}

## What is the FAIaS‚Äô inclusion guide?

The inclusion guide will take the shape of a handbook with interactive materials and diversity recommendations.  With this guide, we want to contribute to the creation of a more inclusive educational system that will also support the workforce to become more diverse and formed by more heterogeneous groups, and/or that the bias is tackled in other ways.

The inclusion guide, also known as [**FAIaS‚Äôs result 2 or PR2**](https://fosteringai.github.io/project/result2/), is all about inclusion in Artificial Intelligence and Education.  It will provide (non-formal) educators with tools and knowledge about bias in education and bias in AI algorithms, and explain with examples and easy-to-use materials why diversity and gender/racial balance is so important in the AI and Education fields and it will include practical tips to tackle this bias challenge together.

## And, how is the development of the inclusion guide going so far?

To achieve the end result, some initial and intermediate steps need to be taken and have been taken.

* In June 2021, CollectiveUP (link to faias page for collectiveUP) prepared the outline and plan for the development of this project results as PR2 coordinator.
The plan was presented to all partners in Madrid (external link to the official page of the city) at the project meeting organized by FAIaS lead partner URJC (link to official page of the university).
* All partners (URJC, TBC, VUB and CU ‚Äì add links to all internal pages of the partners) brainstormed on the plan and agreed to some next steps.  On one hand, to work closely with (non-formal) educators on the topic of bias and interview them, understand their needs, and on the other hand, start the development of materials to tackle the topic of unconscious bias and bias in AI.
* In relation to project result 1 (add link to PR1 internal webpage), a lesson plan was outlined by the AI Lab at VUB (link to external website) to present it to teachers from Braga and to test it out.
* Partner [**Theatro Circo de Braga**](https://fosteringai.github.io/partners/teatro/) carried out focus groups, and interviews with teachers to gather feedback about the lesson plan on bias.  Some conclusions were made, and they can be found in a [**previous post**](https://fosteringai.github.io/post/working_on_result1/)
* As PR1 and PR2 are interrelated in some few aspects, the lesson plan on bias created for PR1 was presented to non-formal educators in Belgium to evaluate if it was feasible for them to use it as it is.  Those initial talks took place online in the months of June, July, September and October 2021 and they were very beneficial to gather some initial needs from these organizations and educators and understand how this PR2 could be developed for them.
* On the other hand, and in direct relation to PR2, CollectiveUP dedicated the months of October and November 2021, to research the topic of unconscious bias and prepared materials for a theoretical and practical workshop for educators.
* In November 2021, the materials and workshop was tested out with teachers and feedback was received on how to integrate the bias topic into formal education lessons and into non-formal activities.
* In December 2021 and January 2022, some extra talks with (non-formal) educators across Europe who already use AI in the classroom have taken place, and we have gathered inspiration from their own lessons!

As next steps:
* Deep interviews with educators will take place in the coming months, and we will record them, edit them and post them on our [**youtube channel**](https://www.youtube.com/channel/UCHC2vEydwNXDEGaLuH_pxQg) and publish the text on our blog as new posts. 
* We will also create a new post that summarizes the initial conclusion from the talks with educators that took place in June-to-October.
* We will publish on our blog the bias materials and workshop that have been developed so far, and the feedback from teachers.
* We will develop examples that use the [**LearningML tool**](https://fosteringai.github.io/post/learningml/) to illustrate the concept of bias in a practical way, and incorporate them as extra materials to complement the workshop on unconscious bias already given in Braga.

Remember to follow us on social media and on our newsletter to keep up-to-date on FAIaS developments!


### [üì¨ Subscribe to our newsletter](http://eepurl.com/hLgTQz) Sing up to receive more information about FAIaS project via email, and you‚Äôll be the first to know about Artificial Intelligence and more.



